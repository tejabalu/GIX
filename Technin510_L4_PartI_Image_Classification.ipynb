{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Technin510_L4_PartI_Image_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejabalu/udub/blob/master/Technin510_L4_PartI_Image_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NseBwf7G1t9P"
      },
      "source": [
        "# LAB 4: Image Classification\n",
        "\n",
        "## Step 1: Create project/folder and download data\n",
        "\n",
        "Download [lab-4.zip](https://drive.google.com/file/d/1vjt-_R1YnpIkxoytykU_ZZ5CLNP7fVgi/view?usp=sharing) and unzip it. Place the `train/` and `test/` folders into `lab-4/` folder you create on your Google drive. These folders include images of five 'symbol' cards as seen from a small robot's camera. Also copy `lab4.ipynb` into the `lab-4/`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqgTPZYE3AgX"
      },
      "source": [
        "## Step 2: Implement feature extraction\n",
        "\n",
        "Below is a skeleton code for an image classification class called ImageClassifier as well as code for creating, training, and testing a classifier with the provided data sets. The three functions you will need to implement are indicated with comments in the code.\n",
        "\n",
        "The first one of these is `extract_image_features` which should return a Numpy array that contains the features that represent the image. Before extracting any features, you should apply Gaussian blurring to your images to get rid of random sensor noice that is common in many lower cost cameras. For this, look into the filters module of scikit-image. Then explore at least two different types of features provided in the feature module of scikit-image. Inspect the size of the feature vectors generated by different methods and what the features look like for different images from the training or testing set. \n",
        "\n",
        "You will not yet get a good sense of how well each feature performs in allowing the classifiers to discriminate between different images. Hence, keep your code for extracting different features around until you explore classification performance in the next step. You can do that by duplicating the function with different names for different features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S99lms1i1BPE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68124bcf-674d-4fba-a164-e092156a415c"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "from sklearn import svm, metrics\n",
        "from skimage import io, feature, filters, exposure, color, exposure\n",
        "from skimage.feature import hog\n",
        "from skimage.feature import canny\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class FeatureExtractor:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.classifier = None\n",
        "        self.folder = '/content/drive/My Drive/lab4-data/lab4-data/'\n",
        "\n",
        "    def imread_convert(self, f):\n",
        "        return io.imread(f).astype(np.uint8)\n",
        "\n",
        "    def save_classifier(self):\n",
        "        joblib.dump(self.classifier, self.folder + 'classifier.joblib')\n",
        "\n",
        "    def load_data_from_folder(self, dir):\n",
        "        # read all images into an image collection\n",
        "        ic = io.ImageCollection(self.folder + dir + '*.bmp',\n",
        "                                load_func=self.imread_convert)\n",
        "\n",
        "        # create one large array of image data\n",
        "        data = io.concatenate_images(ic)\n",
        "        \n",
        "        # extract labels from image names\n",
        "        labels = np.array(ic.files)\n",
        "        for i, f in enumerate(labels):\n",
        "            m = re.search('_', f)\n",
        "            labels[i] = (f[len(dir):m.start()]).split('/')[-1]\n",
        "        \n",
        "        return(data,labels)\n",
        "\n",
        "    def extract_image_features(self, data, feature):\n",
        "        \n",
        "        \n",
        "      \n",
        "        # apply greayscale \n",
        "        # grayscale = rgb2gray(data)\n",
        "\n",
        "        # apply gaussian\n",
        "        filtered_images = filters.gaussian(data)\n",
        "\n",
        "        # print(filtered_images.shape)\n",
        "\n",
        "        featured_images = []\n",
        "\n",
        "\n",
        "\n",
        "        for img in data:\n",
        "          featured_image = feature(img[:,:,0]).flatten()\n",
        "          featured_images.append(featured_image)\n",
        "\n",
        "        featured_images = np.array(featured_images)\n",
        "\n",
        "        return featured_images"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPG2wS285ytL"
      },
      "source": [
        "Here is the code for creating an instance of the class, loading the data, and extracting the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQumlfXtt5hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9cb7ad-b729-411b-fbdb-6e266a38c0d9"
      },
      "source": [
        "img_clf = FeatureExtractor()\n",
        "\n",
        "# load images\n",
        "print('Loading training set...')\n",
        "(train_raw, train_labels) = img_clf.load_data_from_folder('train/')\n",
        "print('Loading testing set...')\n",
        "(test_raw, test_labels) = img_clf.load_data_from_folder('test/')\n",
        "print()\n",
        "\n",
        "# convert images into features, example using hog features\n",
        "print('Extracting HOG features...')\n",
        "train_data_hog = img_clf.extract_image_features(train_raw, hog)\n",
        "test_data_hog = img_clf.extract_image_features(test_raw, hog)\n",
        "\n",
        "# repeat with at least one other feature type...\n",
        "train_data_canny = img_clf.extract_image_features(train_raw, canny)\n",
        "test_data_canny = img_clf.extract_image_features(test_raw, canny)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training set...\n",
            "Loading testing set...\n",
            "\n",
            "Extracting HOG features...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHW9Rr1O5peP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1413b409-c85a-4d07-9570-7d82cee144a4"
      },
      "source": [
        "# Inspecting the features.\n",
        "print('HOG Feature Vector')\n",
        "print(train_data_hog)\n",
        "print(f'Vector Size: {len(train_data_hog[0])}')\n",
        "print(train_data_hog.shape)\n",
        "\n",
        "print('Canny Feature Vector')\n",
        "print(train_data_canny)\n",
        "print(f'Vector Size: {len(train_data_canny[0])}')\n",
        "print(train_data_canny.shape)\n",
        "\n",
        "# repeat with at least one other feature type..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOG Feature Vector\n",
            "[[0.04938107 0.0259555  0.02685569 ... 0.01233706 0.00973884 0.03351848]\n",
            " [0.03564477 0.00291106 0.01110077 ... 0.         0.         0.00267414]\n",
            " [0.01676485 0.002603   0.00647903 ... 0.         0.         0.01637841]\n",
            " ...\n",
            " [0.01852383 0.00806496 0.00464882 ... 0.00540789 0.01340184 0.0111444 ]\n",
            " [0.05097509 0.00609174 0.01561055 ... 0.         0.         0.01646256]\n",
            " [0.06646614 0.02353476 0.01289356 ... 0.         0.         0.        ]]\n",
            "Vector Size: 86184\n",
            "(196, 86184)\n",
            "Canny Feature Vector\n",
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "Vector Size: 76800\n",
            "(196, 76800)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co_et4A-58X5"
      },
      "source": [
        "## Step 3: Implement classifier training and testing\n",
        "Next, implement the `train_classifier` and `predict_labels` functions for at least two different types of classifiers. After this, the second cell below shoul then produce performance results of the classifier.\n",
        "\n",
        "Explore the performance of at least **two feature types** and at least **two classifiers** (i.e. at least four different combinations) in terms of classification **performance** on the test set. Update the code to display the **F1 score** on the test set for the **four different combinations** with informative prompts. Then the code should display the **detailed performance** (confusion matrix, accuracy, F1 score) that is already displayed only for the best performing combination of features and classifier. Also make sure the best performing classifier is saved onto your drive for the next step of the lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHzU_eUg5K7V"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import f1_score \n",
        "\n",
        "# Continue implementation of the ImageClassifier class in this cell\n",
        "class ImageClassifier(FeatureExtractor): # we are linking both the classes imgvclassifier has all the methods from the featureextractor \n",
        "\n",
        "    def train_classifier(self, train_data, train_labels, classifier):\n",
        "        \n",
        "        self.classifier = classifier()\n",
        "        self.classifier.fit(train_data, train_labels)\n",
        "\n",
        "#here train data will be the features from the above \n",
        "    def predict_labels(self, data):\n",
        "        \n",
        "        predicted_labels = self.classifier.predict(data)\n",
        "        \n",
        "        return predicted_labels        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzu432Q86AMR"
      },
      "source": [
        "Re-run initialization and feature extraction, then train/test the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44IFo86c6J-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3890f03-0d2e-4f30-9962-6e219732bc66"
      },
      "source": [
        "img_clf = ImageClassifier()\n",
        "\n",
        "# create list of list containing features you want to run through along with any\n",
        "# associated metadata\n",
        "\n",
        "# features = \n",
        "features = [hog, canny]\n",
        "# testfeatures = [test_data_hog , test_data_canny]\n",
        "\n",
        "# create list of list of classifiers you want to use \n",
        "\n",
        "# classifiers = \n",
        "classifiers = [KNeighborsClassifier, MLPClassifier, DecisionTreeClassifier, GaussianNB]\n",
        "\n",
        "# Variable to hold the best F1 score\n",
        "best_f1 = 0  \n",
        "\n",
        "# Loop through the different combinantions of features and classifiers\n",
        "for i in features:\n",
        "\n",
        "  # extract features for train raw using current feature value\n",
        "  itrain = img_clf.extract_image_features(train_raw, i)\n",
        "  itest = img_clf.extract_image_features(test_raw, i)\n",
        "  # extract features for test raw using curent feature value\n",
        "  # (What we know: train_labels and test_labels from above)\n",
        "  for classifier in classifiers:  \n",
        "    \n",
        "\n",
        "    # Train model\n",
        "    model = img_clf.train_classifier(itrain, train_labels, classifier)\n",
        "\n",
        "    # Test model\n",
        "    predictetdlabels = img_clf.predict_labels(itest)\n",
        "    \n",
        "    # Create confusion matrix\n",
        "    confmatrx = confusion_matrix(test_labels, predictetdlabels)\n",
        "    print(confmatrx)\n",
        "\n",
        "    # Print test accuracy\n",
        "    accscore = accuracy_score(test_labels, predictetdlabels)\n",
        "    print(accscore)\n",
        "\n",
        "    # Print test F1 score\n",
        "    f1 = f1_score(test_labels, predictetdlabels, average='macro')\n",
        "    print(f1)\n",
        "    print()\n",
        "    \n",
        "    # Check if last model is better than the current best one and save it\n",
        "    if f1>best_f1:\n",
        "      best_f1=f1\n",
        "      img_clf.save_classifier()\n",
        "      \n",
        "    \n",
        "    # pass # delete this line"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 1 0 0 1 0 0 1]\n",
            " [0 5 0 0 0 0 0 0]\n",
            " [0 1 4 0 0 0 0 0]\n",
            " [0 0 1 4 0 0 0 0]\n",
            " [1 0 0 0 4 0 0 0]\n",
            " [0 0 0 0 1 2 2 0]\n",
            " [0 0 1 0 0 0 4 0]\n",
            " [0 0 0 0 0 0 0 5]]\n",
            "0.75\n",
            "0.7355699855699855\n",
            "\n",
            "[[5 0 0 0 0 0 0 0]\n",
            " [0 5 0 0 0 0 0 0]\n",
            " [0 0 5 0 0 0 0 0]\n",
            " [0 0 1 4 0 0 0 0]\n",
            " [0 0 0 0 5 0 0 0]\n",
            " [0 0 0 0 1 4 0 0]\n",
            " [0 0 0 0 0 0 5 0]\n",
            " [0 0 0 0 0 0 0 5]]\n",
            "0.95\n",
            "0.9494949494949495\n",
            "\n",
            "[[3 1 1 0 0 0 0 0]\n",
            " [0 1 2 2 0 0 0 0]\n",
            " [0 0 2 0 2 0 1 0]\n",
            " [0 1 1 3 0 0 0 0]\n",
            " [2 0 0 0 1 2 0 0]\n",
            " [0 0 0 1 1 3 0 0]\n",
            " [1 0 0 1 0 1 2 0]\n",
            " [1 0 0 1 0 1 0 2]]\n",
            "0.425\n",
            "0.42110320235320237\n",
            "\n",
            "[[0 0 0 5 0 0 0 0]\n",
            " [0 1 0 4 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 2 0 1 2 0]\n",
            " [0 0 0 1 0 4 0 0]\n",
            " [0 0 0 1 0 0 4 0]\n",
            " [0 0 0 4 0 0 0 1]]\n",
            "0.375\n",
            "0.3133049242424243\n",
            "\n",
            "[[0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]]\n",
            "0.125\n",
            "0.027777777777777776\n",
            "\n",
            "[[5 0 0 0 0 0 0 0]\n",
            " [3 0 0 0 0 2 0 0]\n",
            " [3 0 0 0 0 2 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [1 0 0 0 0 4 0 0]\n",
            " [1 0 0 0 0 4 0 0]\n",
            " [5 0 0 0 0 0 0 0]\n",
            " [3 0 0 0 0 2 0 0]]\n",
            "0.35\n",
            "0.2257085020242915\n",
            "\n",
            "[[4 0 0 0 0 1 0 0]\n",
            " [0 1 1 0 0 1 1 1]\n",
            " [2 0 1 1 1 0 0 0]\n",
            " [3 1 0 1 0 0 0 0]\n",
            " [3 1 1 0 0 0 0 0]\n",
            " [0 1 0 2 1 1 0 0]\n",
            " [0 3 0 0 1 1 0 0]\n",
            " [2 2 0 1 0 0 0 0]]\n",
            "0.2\n",
            "0.15451649958228908\n",
            "\n",
            "[[0 2 0 0 3 0 0 0]\n",
            " [0 0 2 0 3 0 0 0]\n",
            " [0 2 0 0 2 1 0 0]\n",
            " [0 0 0 4 1 0 0 0]\n",
            " [0 0 0 0 5 0 0 0]\n",
            " [0 0 0 0 4 1 0 0]\n",
            " [0 0 0 0 5 0 0 0]\n",
            " [0 0 0 0 4 1 0 0]]\n",
            "0.25\n",
            "0.1814236111111111\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DBUw_P1eRBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339435f5-8908-40c5-892e-7e30bd33d833"
      },
      "source": [
        "classifiers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[sklearn.neighbors._classification.KNeighborsClassifier,\n",
              " sklearn.neural_network._multilayer_perceptron.MLPClassifier,\n",
              " sklearn.tree._classes.DecisionTreeClassifier,\n",
              " sklearn.naive_bayes.GaussianNB]"
            ]
          },
          "metadata": {},
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxpmL5GSZL3p"
      },
      "source": [
        "# TODO: Save the best model using the `save_classifier` method of the `FeatureExtractor` class."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF_Qu0I9O0zl"
      },
      "source": [
        "## Step 4: Transfer classifier to your camera\n",
        "\n",
        "Next you will apply your trained classifier directly onto images captured by your webcam. Since this is our last lab using Python we would like to give you the opportunity to install Python and Jupyter notebooks on your machine, and do this part of the lab as a conventional Python script. However, if you would rather not do that at the moment, you still have the option of doing this part with Colab Notebooks.\n",
        "\n",
        "* **Option 1:** If you would like to do this part locally, first follow the Python Installation guidelines. Then download the skeleton camera capture script [`camera.py`](https://drive.google.com/file/d/1jmll8_rsagFLcaaIpKtYgCd2Rn8rVXlL/view?usp=sharing) and test it out. Then update this script as described below and submit this script. Make sure you note in lab4.ipynb that you chose this option.\n",
        "* **Option 2:** If you would like to do this part in Colab Notebooks, add your code below. Check out [`camera.ipynb`](https://colab.research.google.com/drive/1IfHqK83dDVyxsQzQwsnxc4CUrRUoL9y8) for sample code for capturing camera images in Colab notebooks.\n",
        "\n",
        "Your code should first load the classifier you saved in Step 3 with the following line (already implemented in the sample code):\n",
        "\n",
        "`classifier = joblib.load('classifier.joblib')`\n",
        "\n",
        "The script should then go into a loop where it (1) captures a new image from the camera, (2) processes the image to make it grayscale and filter the noise, (3) extracts the features like you did in Step 2 using the right set of features for your trained classifier, (4) detects whether the image contains one of the seven images using the trained classifier, and (5) displays the detected class name on the image in every iteration. You can use images printed on paper or displayed on your phone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxAIUMPmPRY-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "outputId": "f8c71229-fc67-4a7f-d1d3-d1e4ca59102b"
      },
      "source": [
        "from IPython.display import HTML, Audio\n",
        "from IPython.display import clear_output\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import io as io2\n",
        "from PIL import Image\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "############################\n",
        "####### CAMERA CODE ########\n",
        "############################\n",
        "\n",
        "VIDEO_HTML = \"\"\"\n",
        "<video autoplay\n",
        " width=%d height=%d style='cursor: pointer;'></video>\n",
        "<script>\n",
        "\n",
        "var video = document.querySelector('video')\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({ video: true })\n",
        "  .then(stream=> video.srcObject = stream)\n",
        "\n",
        "function getFrame() {\n",
        "    var canvas = document.createElement('canvas')\n",
        "    var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
        "    canvas.width = w\n",
        "    canvas.height = h\n",
        "    canvas.getContext('2d')\n",
        "          .drawImage(video, 0, 0, w, h)\n",
        "    return canvas.toDataURL('image/jpeg', 0.8)\n",
        "}\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Make sure the pictures taken by your camera match the size of the \n",
        "# training set – 320px x 240px\n",
        "def start_camera(filename='photo.jpg', quality=0.8, size=(320,240)):\n",
        "  display(HTML(VIDEO_HTML % (size[0],size[1])))\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8, size=(320,240)):\n",
        "  data = eval_js('getFrame()')\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  f = io2.BytesIO(binary)\n",
        "  return np.asarray(Image.open(f))\n",
        "\n",
        "############################\n",
        "############################\n",
        "############################\n",
        "\n",
        "class ImageClassifer_webcam(ImageClassifier):\n",
        "  \n",
        "  def load_classifier(self):\n",
        "    print('Loading classifier...')\n",
        "    self.classifier = joblib.load(self.folder + 'classifier.joblib')\n",
        "    print(self.classifier)\n",
        "\n",
        "# Create a new instance of the classifier\n",
        "img_clf = ImageClassifer_webcam()\n",
        "\n",
        "# Load the previously saved model\n",
        "img_clf.load_classifier()\n",
        "\n",
        "# Start camera and wait for it to \"warm up\"\n",
        "start_camera()\n",
        "time.sleep(3)\n",
        "\n",
        "while(True):\n",
        "  img = take_photo() # click\n",
        "\n",
        "  ##############################################################################\n",
        "  ############################ YOUR CODE HERE ##################################\n",
        "  ##############################################################################\n",
        "\n",
        "  # Convert to grayscale\n",
        "  grayscale = rgb2gray(img)\n",
        "  print(grayscale.shape)\n",
        "\n",
        "\n",
        "  # Extract features from the image just caputred\n",
        "  imgf = hog(grayscale)\n",
        "\n",
        "  # Show frame and prediction\n",
        "  pred = img_clf.predict_labels(imgf.reshape(1, -1))\n",
        "  print(pred)\n",
        "  \n",
        "  time.sleep(5)\n",
        "\n",
        "  # break # delete this line\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading classifier...\n",
            "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
            "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "              warm_start=False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<video autoplay\n",
              " width=320 height=240 style='cursor: pointer;'></video>\n",
              "<script>\n",
              "\n",
              "var video = document.querySelector('video')\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({ video: true })\n",
              "  .then(stream=> video.srcObject = stream)\n",
              "\n",
              "function getFrame() {\n",
              "    var canvas = document.createElement('canvas')\n",
              "    var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
              "    canvas.width = w\n",
              "    canvas.height = h\n",
              "    canvas.getContext('2d')\n",
              "          .drawImage(video, 0, 0, w, h)\n",
              "    return canvas.toDataURL('image/jpeg', 0.8)\n",
              "}\n",
              "\n",
              "</script>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 320)\n",
            "['none']\n",
            "(240, 320)\n",
            "['none']\n",
            "(240, 320)\n",
            "['none']\n",
            "(240, 320)\n",
            "['none']\n",
            "(240, 320)\n",
            "['none']\n",
            "(240, 320)\n",
            "['none']\n",
            "(240, 320)\n",
            "['order']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-364-678214ec1982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0;31m# break # delete this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hRob3q_Qedo"
      },
      "source": [
        "## (Optional) Improve classifier\n",
        "\n",
        "You will notice that your classifier is prone to errors when tested with your camera. Part of the reason is that the images were collected from a different camera. You can try improving the performance of your camera image classifier by re-training your classifier with images collected from your camera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxJ9SZubQjKl"
      },
      "source": [
        "## Step 5: Submit your code on Canvas\n",
        "\n",
        "Complete this lab by submitting a link to your updated Colab Notebook (and if you chose Option 1 in Step 4, by uploading your updated `camera.py`) on Canvas, by Oct 22 Tuesday, 11:59pm. We will test your code by running it and inspecting the classification results to make sure: \n",
        "* A comparison of  at least four combinations of feature types and classifiers were made\n",
        "* A reasonable classification performance was achieved with the best combination (higher than random chance).\n",
        "\n",
        "We will test your camera image classification code by running it and showing it the seven different printed images to check that more than half of the images can be recognized correctly in some configuration relative to the camera.\n",
        "\n",
        "See Canvas for a grading rubric."
      ]
    }
  ]
}